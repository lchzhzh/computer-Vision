import random
import numpy as np
import matplotlib.pyplot as plt

N=100 #number of points per class
D=2 #dimensionally
K=3 #number of class
X=np.zeros((N*K,D)) #data matrix (each row = single example)
y=np.zeros(N*K,dtype='uint8') #class label
for j in range (K): 
    ix=range(N*j,N*(j+1))
    r=np.linspace(0.0,1,N) #radius
    t=np.linspace(j*4,(j+1)*4,N)+np.random.randn(N)*0.2 #theta
    X[ix]=np.c_[r*np.sin(t),r*np.cos(t)]
    y[ix]=j
# lets visualize the data
#plt.scatter(X[:,0],X[:,1],c=y,s=40,cmap=plt.cm.Spectral)
#plt.show()

# initialize parameters randomly
W=0.01*np.random.randn(D,K)
b=np.zeros((1,K))

# some hyperparameters 
step_size=1e-0

reg=1e-3 #regularization strength

#gradient desent loop
num_examples=X.shape[0]  
for i in range(200):

    #evaluate class scores,[N*K]
    scores=np.dot(X,W)+b
    #compute the class probabilities
    exp_scores=np.exp(scores)  #size [300*3]
    probs=exp_scores/np.sum(exp_scores,axis=1,keepdims=True) #size [300*3]
    # compute the loops:average cross-entropy loss and regulation
    correct_logprobs=-np.log(probs[range(num_examples),y])
    data_loss=np.sum(correct_logprobs)/num_examples
    reg_loss=0.5*reg*np.sum(W*W)
    loss=data_loss+reg_loss
    if i%10==0:
        print (i ,loss) 
    dscore=probs
    dscore[range(num_examples),y]-=1
    dscore/=num_examples

    #backpropate the gradient to the parameters(W,b)
    dW=np.dot(X.T,dscore)
    db=np.sum(dscore,axis=0,keepdims=True)

    dW+=reg*W

    #perform a parameter update

    W+=-step_size*dW
    b+=-step_size*db

    # evaluate training set accuracy
    
    scores=np.dot(X,W)+b
predicted_class=np.argmax(scores,axis=1)
print(scores)
print(predicted_class)
print(np.mean(predicted_class==y))
